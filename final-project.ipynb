{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Outbound Reach Rate of a Call Center \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Course: Machine Learning\n",
    "    Instructor: Mehmet GÃ¶nen\n",
    "    \n",
    "    Name : Waris Gill\n",
    "    ID   : 0067664\n",
    "    \n",
    "    For better results check the html file or run jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages required for the Assignment\n",
    "(I have used some of the packages in the assignment, and other are just for testing and for experimentation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import model_selection\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfx = pd.read_csv(\"training_data.csv\")\n",
    "dfy = pd.read_csv(\"training_labels.csv\",header=None)\n",
    "X = dfx.values\n",
    "Y = dfy.values\n",
    "\n",
    "seed = 7\n",
    "scoring = 'roc_auc' # scoring parameter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Dimensionality Reduction\n",
    "The one of the most important step in ML experiments is to extract the useful feature from the given features. By reducing the number of features our model will run faster and will have better performance. So, to extract useful features I have tried different algorithms and for our data I have found that decesion tree based feature extraction perfoms really good and run faster. Below code ranks the features from highest to lowest i.e 1st feature is more importance as compared the 2nd and so on. \n",
    "\n",
    "And I have found that first top 8 to 12 are enough to train our model but I have used more than these features to see the performance and score but it did not make any difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 69 (0.076693)\n",
      "2. feature 73 (0.072393)\n",
      "3. feature 71 (0.070356)\n",
      "4. feature 1 (0.066811)\n",
      "5. feature 0 (0.054096)\n",
      "6. feature 123 (0.038272)\n",
      "7. feature 68 (0.037923)\n",
      "8. feature 72 (0.035266)\n",
      "9. feature 122 (0.032122)\n",
      "10. feature 70 (0.027352)\n",
      "11. feature 35 (0.013530)\n",
      "12. feature 74 (0.013431)\n",
      "13. feature 6 (0.013163)\n",
      "14. feature 4 (0.011273)\n",
      "15. feature 98 (0.010585)\n",
      "16. feature 127 (0.010159)\n",
      "17. feature 80 (0.009836)\n",
      "18. feature 133 (0.009655)\n",
      "19. feature 97 (0.009535)\n",
      "20. feature 131 (0.009386)\n",
      "21. feature 8 (0.009248)\n",
      "22. feature 130 (0.009083)\n",
      "23. feature 132 (0.009051)\n",
      "24. feature 134 (0.009011)\n",
      "25. feature 5 (0.008996)\n",
      "26. feature 126 (0.008917)\n",
      "27. feature 104 (0.008914)\n",
      "28. feature 125 (0.008847)\n",
      "29. feature 79 (0.008841)\n",
      "30. feature 128 (0.008838)\n",
      "31. feature 124 (0.008810)\n",
      "32. feature 116 (0.008510)\n",
      "33. feature 95 (0.008471)\n",
      "34. feature 91 (0.008217)\n",
      "35. feature 103 (0.008190)\n",
      "36. feature 139 (0.008121)\n",
      "37. feature 88 (0.008106)\n",
      "38. feature 138 (0.008105)\n",
      "39. feature 21 (0.007941)\n",
      "40. feature 140 (0.007896)\n",
      "41. feature 87 (0.007852)\n",
      "42. feature 9 (0.007353)\n",
      "43. feature 11 (0.007295)\n",
      "44. feature 96 (0.007280)\n",
      "45. feature 136 (0.006939)\n",
      "46. feature 101 (0.006896)\n",
      "47. feature 137 (0.006893)\n",
      "48. feature 117 (0.006835)\n",
      "49. feature 106 (0.006535)\n",
      "50. feature 57 (0.006435)\n",
      "51. feature 86 (0.006202)\n",
      "52. feature 67 (0.005932)\n",
      "53. feature 112 (0.005896)\n",
      "54. feature 135 (0.005881)\n",
      "55. feature 111 (0.005857)\n",
      "56. feature 90 (0.005417)\n",
      "57. feature 129 (0.005371)\n",
      "58. feature 85 (0.005064)\n",
      "59. feature 17 (0.004417)\n",
      "60. feature 115 (0.004116)\n",
      "61. feature 49 (0.003962)\n",
      "62. feature 60 (0.003686)\n",
      "63. feature 63 (0.003669)\n",
      "64. feature 83 (0.003389)\n",
      "65. feature 141 (0.003261)\n",
      "66. feature 62 (0.003218)\n",
      "67. feature 48 (0.003202)\n",
      "68. feature 44 (0.002912)\n",
      "69. feature 32 (0.002691)\n",
      "70. feature 7 (0.002228)\n",
      "71. feature 118 (0.002207)\n",
      "72. feature 77 (0.002077)\n",
      "73. feature 28 (0.001976)\n",
      "74. feature 76 (0.001971)\n",
      "75. feature 43 (0.001971)\n",
      "76. feature 99 (0.001940)\n",
      "77. feature 58 (0.001901)\n",
      "78. feature 29 (0.001761)\n",
      "79. feature 18 (0.001736)\n",
      "80. feature 108 (0.001704)\n",
      "81. feature 16 (0.001545)\n",
      "82. feature 100 (0.001517)\n",
      "83. feature 34 (0.001505)\n",
      "84. feature 13 (0.001384)\n",
      "85. feature 114 (0.001349)\n",
      "86. feature 23 (0.001273)\n",
      "87. feature 110 (0.001184)\n",
      "88. feature 119 (0.001159)\n",
      "89. feature 19 (0.001049)\n",
      "90. feature 84 (0.000977)\n",
      "91. feature 56 (0.000916)\n",
      "92. feature 78 (0.000903)\n",
      "93. feature 40 (0.000901)\n",
      "94. feature 36 (0.000858)\n",
      "95. feature 75 (0.000857)\n",
      "96. feature 121 (0.000812)\n",
      "97. feature 47 (0.000791)\n",
      "98. feature 38 (0.000747)\n",
      "99. feature 59 (0.000720)\n",
      "100. feature 65 (0.000619)\n",
      "101. feature 92 (0.000603)\n",
      "102. feature 52 (0.000546)\n",
      "103. feature 15 (0.000535)\n",
      "104. feature 45 (0.000515)\n",
      "105. feature 55 (0.000500)\n",
      "106. feature 53 (0.000495)\n",
      "107. feature 89 (0.000481)\n",
      "108. feature 82 (0.000346)\n",
      "109. feature 27 (0.000346)\n",
      "110. feature 22 (0.000339)\n",
      "111. feature 12 (0.000336)\n",
      "112. feature 24 (0.000321)\n",
      "113. feature 2 (0.000306)\n",
      "114. feature 120 (0.000306)\n",
      "115. feature 51 (0.000292)\n",
      "116. feature 94 (0.000275)\n",
      "117. feature 26 (0.000245)\n",
      "118. feature 42 (0.000207)\n",
      "119. feature 25 (0.000189)\n",
      "120. feature 39 (0.000183)\n",
      "121. feature 14 (0.000181)\n",
      "122. feature 37 (0.000171)\n",
      "123. feature 54 (0.000168)\n",
      "124. feature 30 (0.000164)\n",
      "125. feature 20 (0.000154)\n",
      "126. feature 64 (0.000141)\n",
      "127. feature 66 (0.000110)\n",
      "128. feature 81 (0.000109)\n",
      "129. feature 3 (0.000101)\n",
      "130. feature 31 (0.000093)\n",
      "131. feature 50 (0.000052)\n",
      "132. feature 61 (0.000041)\n",
      "133. feature 10 (0.000037)\n",
      "134. feature 107 (0.000037)\n",
      "135. feature 93 (0.000032)\n",
      "136. feature 102 (0.000028)\n",
      "137. feature 46 (0.000020)\n",
      "138. feature 105 (0.000017)\n",
      "139. feature 109 (0.000010)\n",
      "140. feature 33 (0.000009)\n",
      "141. feature 41 (0.000005)\n",
      "142. feature 113 (0.000004)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of features  : [ 69  73  71   1   0 123  68  72 122  70  35  74   6   4  98 127  80 133\n",
      "  97 131   8 130 132 134   5 126 104 125  79 128 124 116  95  91 103 139\n",
      "  88 138  21 140  87   9  11  96 136 101 137 117 106  57  86  67 112 135\n",
      " 111  90 129  85  17 115  49  60  63  83 141  62  48  44  32   7 118  77\n",
      "  28  76  43  99  58  29  18 108  16 100  34  13 114  23 110 119  19  84\n",
      "  56  78  40  36  75 121  47  38  59  65  92  52  15  45  55  53  89  82\n",
      "  27  22  12  24   2 120  51  94  26  42  25  39  14  37  54  30  20  64\n",
      "  66  81   3  31  50  61  10 107  93 102  46 105 109  33  41 113]\n"
     ]
    }
   ],
   "source": [
    "num_trees = 64\n",
    "# Build a forest and compute the feature importances\n",
    "forest = RandomForestClassifier(n_estimators= 24,random_state=seed)\n",
    "# forest = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "# RandomForestClassifier(n_estimators=num_trees,random_state=seed, max_depth = 3, min_samples_leaf=4)\n",
    "forest.fit(X, np.ravel(Y))\n",
    "importances = forest.feature_importances_\n",
    "# print(importances)\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()\n",
    "print(\"Indices of features  :\",indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Models\n",
    "Here are the different set of models which I have used during my assignment to find the best model. And after fine tuning of hyperparameters I have found that Stochastic Gradient Boosting (a boosting algorithm which is similar to adaboos) performs better as compared to other algorithms. \n",
    "\n",
    "In all these models I have used scoring parameter = AUROC value because it is required in the assignment. \n",
    "\n",
    "(The predictive quality of your solution will be evaluated in terms of its AUROC value on the test set.)\n",
    "\n",
    "The other faster algorithm was logistic regression it was surprisingly running fast and its AUC was very close to SGB. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bagged Decision Trees for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BDTC(X,Y,num_trees,kfold):\n",
    "    cart = DecisionTreeClassifier()\n",
    "    model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "\n",
    "    results = model_selection.cross_val_score(model, X,np.ravel(Y), cv=kfold, n_jobs=-1,scoring=scoring)\n",
    "    print(results.mean())\n",
    "    print(\"AUC:\", results.mean())\n",
    "    print(\"Std:\", results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFC(X,Y,num_trees,kfold):\n",
    "    max_features = 20 # ignored\n",
    "    model = RandomForestClassifier(n_estimators=num_trees,random_state=seed, max_depth = 3)\n",
    "\n",
    "    results = model_selection.cross_val_score(model, X,np.ravel(Y), cv=kfold, n_jobs=-1,scoring=scoring)\n",
    "#     print(results.mean())\n",
    "#     print(\"AUC:\", results.mean())\n",
    "#     print(\"Std:\", results.std())\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EXT(X,Y,num_trees,kfold):\n",
    "    model = ExtraTreesClassifier(n_estimators=num_trees,random_state=seed)\n",
    "    results = model_selection.cross_val_score(model, X,np.ravel(Y), cv=kfold, n_jobs=-1,scoring=scoring)\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGB (X,Y,num_trees,kfold): \n",
    "    model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "    results = model_selection.cross_val_score(model, X,np.ravel(Y), cv=kfold, n_jobs=-1,scoring=scoring)\n",
    "#     print(results.mean())\n",
    "#     print(\"AUC:\", results.mean())\n",
    "#     print(\"Std:\", results.std())\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting(X,Y,num_trees,kfold):\n",
    "    # create the sub models\n",
    "    estimators = []\n",
    "    \n",
    "    model1 = LogisticRegression()\n",
    "#     model2 = QuadraticDiscriminantAnalysis()\n",
    "#     model3 = GaussianNB()\n",
    "#     model4 = DecisionTreeClassifier(random_state=seed,max_depth = 3)\n",
    "    model5 = RandomForestClassifier(n_estimators=num_trees,random_state=seed, max_depth = 3, min_samples_leaf=4)\n",
    "    model6 = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "    \n",
    "    estimators.append(('m1', model1))\n",
    "#     estimators.append(('m2', model2))  \n",
    "#     estimators.append(('m3', model3))\n",
    "# #     estimators.append(('m4', model4))\n",
    "    estimators.append(('m5', model5))\n",
    "    estimators.append(('m6', model6))\n",
    "    \n",
    "    \n",
    "    # create the ensemble model\n",
    "    model = VotingClassifier(estimators,voting=\"soft\")\n",
    "    results = model_selection.cross_val_score(model, X,np.ravel(Y), cv=kfold, n_jobs=-1,scoring=scoring)\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6. More Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo1_LogisticRegression(X,Y,kfold):    \n",
    "#     print(\"Features \",X.shape[1])\n",
    "    model = LogisticRegression()\n",
    "    results = model_selection.cross_val_score(model, X,np.ravel(Y), cv=kfold, n_jobs=-1,scoring=scoring)\n",
    "    print(\"Logistic Reg :\", results.mean())\n",
    "    return results.mean()\n",
    "\n",
    "\n",
    "def algo2_QDA(X,Y,kfold):    \n",
    "#     print(\"Features \",X.shape[1])\n",
    "    model = QuadraticDiscriminantAnalysis()\n",
    "    results = model_selection.cross_val_score(model, X,np.ravel(Y), cv=kfold, n_jobs=-1,scoring=scoring)\n",
    "    print(\"QDA :\", results.mean())\n",
    "    return results.mean()\n",
    "\n",
    "def algo3_GaussianNB(X,Y,kfold):\n",
    "#     print(\"Features \",X.shape[1])\n",
    "    model = GaussianNB()\n",
    "    results = model_selection.cross_val_score(model, X,np.ravel(Y), cv=kfold, n_jobs=-1,scoring=scoring)\n",
    "    print(\"GaussianNB :\", results.mean())\n",
    "    return results.mean()\n",
    "\n",
    "def algo4_tree(X,Y,kfold):\n",
    "    model = DecisionTreeClassifier(random_state=seed,max_depth = 3)\n",
    "    results = model_selection.cross_val_score(model, X,np.ravel(Y), cv=kfold, n_jobs=-1,scoring=scoring)\n",
    "    print(\"Tree :\", results.mean())\n",
    "    return results.mean()\n",
    "\n",
    "# def algo4_LinearSVC(X,Y,kfold):\n",
    "#     model = LinearSVC(random_state=seed, tol=1e-5)\n",
    "#     results = model_selection.cross_val_score(model, X,np.ravel(Y), cv=kfold, n_jobs=-1,scoring=scoring)\n",
    "#     print(\"Linear SVC :\", results.mean())\n",
    "#     return results.mean()\n",
    "\n",
    "# def algo5_SVC(X,Y,kfold):\n",
    "# #     print(\"Features \",X.shape[1])\n",
    "#     model = SVC(gamma='auto')\n",
    "#     results = model_selection.cross_val_score(model, X,np.ravel(Y), cv=kfold, n_jobs=-1,scoring=scoring)\n",
    "#     print(\"SVC :\", results.mean())\n",
    "#     return results.mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model With Optimize set of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************  More Accurate Solution **********************\n",
      "[[0.66267808 0.33732192]\n",
      " [0.81379426 0.18620574]\n",
      " [0.97679922 0.02320078]\n",
      " ...\n",
      " [0.90881294 0.09118706]\n",
      " [0.91880553 0.08119447]\n",
      " [0.91880553 0.08119447]]\n",
      "AUC =  0.7629894190764027\n",
      "Probabilities on the test data are written to the file  :  predicted_probs.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"****************  More Accurate Solution **********************\")\n",
    "\n",
    "num_features = 36\n",
    "selected_feature_indices=indices[:num_features]\n",
    "folds = 10\n",
    "num_trees = 64\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(n_splits=folds, random_state=None, shuffle=True)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "param_grid = {'n_estimators': [64]}\n",
    "\n",
    "grid_clf = GridSearchCV(estimator=model,param_grid =param_grid ,cv=kfold,scoring=scoring)\n",
    "grid_clf.fit(np.take(X,selected_feature_indices,axis=1),np.ravel(Y))\n",
    "\n",
    "Xtest = pd.read_csv(\"test_data.csv\").values\n",
    "Xtest=np.take(Xtest,selected_feature_indices,axis=1)\n",
    "final_probs = grid_clf.predict_proba(Xtest)\n",
    "\n",
    "file_name = \"predicted_probs.csv\"\n",
    "np.savetxt(file_name,np.take(final_probs,[1],axis=1),delimiter=\",\")\n",
    "\n",
    "print(final_probs)\n",
    "print(\"AUC = \",grid_clf.best_score_) \n",
    "print(\"Probabilities on the test data are written to the file  : \", file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments With Different Models\n",
    "\n",
    "In this section I have run differnet algorithms to tune the hyper paramerters. You can uncomment the below section to see the different models in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(X,Y,num_trees,folds):\n",
    "    #     print(\"Features: \", X.shape[1])\n",
    "    kfold = model_selection.StratifiedKFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "    \n",
    "    #     algo1_LogisticRegression(X,Y,kfold)\n",
    "    #     algo2_QDA(X,Y,kfold)\n",
    "    #     algo3_GaussianNB(X,Y,kfold)\n",
    "    auc1 = RFC(X,Y,num_trees,kfold)\n",
    "    auc1 = round(auc1,4)\n",
    "    auc2 = SGB(X,Y,num_trees,kfold)\n",
    "    auc2 = round(auc2,4)\n",
    "    #     auc3 = voting(X,Y,num_trees,kfold) \n",
    "    #     auc3 = round(auc3,3)\n",
    "    #     algo2_LinearSVC(X,Y,kfold)\n",
    "    #     algo3_SVC(X,Y,kfold)\n",
    "    print(\"Feature = {feature}, Trees = {trees}, Folds = {folds}, AUC_F = {auc1}, AUC_G={auc2}\".format(\n",
    "        feature=X.shape[1],trees=num_trees,folds=folds,auc1=auc1,auc2=auc2))\n",
    "    \n",
    "    return None\n",
    "\n",
    "    \n",
    "# features_nums = [12,18]\n",
    "# tree_nums = [34,64,128]\n",
    "# kfolds = [10,20]\n",
    "\n",
    "# print(X.shape,Y.shape)\n",
    "# for kf in kfolds: \n",
    "#     print(\"****************** kf = {kf} **************************\".format(kf=kf))\n",
    "#     for t in tree_nums:\n",
    "#         print(\"\\n\\n\")\n",
    "#         for f in features_nums:\n",
    "#             selected_feature_indices=indices[:f]\n",
    "#             comparison(np.take(X,selected_feature_indices,axis=1),Y,t,kf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
